{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b7a49-2641-4bc9-a841-1fa976bb1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26193a20-2eae-411b-bda2-ec927d6cd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, dropout_rate, num_units1, num_units2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, num_filters, kernel_size=kernel_size, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Assuming input size is (3, 64, 64)\n",
    "        self.input_size = 64\n",
    "        self.conv_output_size = self._get_conv_output_size(self.input_size, kernel_size, 2)  # 2 is the pool size\n",
    "\n",
    "        self.fc1 = nn.Linear(num_filters * self.conv_output_size * self.conv_output_size, num_units1)\n",
    "        self.fc2 = nn.Linear(num_units1, num_units2)\n",
    "        self.fc3 = nn.Linear(num_units2, 5)  # 5 classes for airfield, bus stand, canyon, market, temple\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "    def _get_conv_output_size(self, input_size, kernel_size, pool_size):\n",
    "        # Calculate output size after a single conv + pool layer\n",
    "        # Output size after convolution\n",
    "        conv_output_size = (input_size - (kernel_size - 1) - 1 + 2) // 1 + 1\n",
    "        # Output size after pooling\n",
    "        conv_output_size = (conv_output_size - (pool_size - 1) - 1) // pool_size + 1\n",
    "        return conv_output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd25f21-7628-423d-8d05-e9ebcfab726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root_dir, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),  # Resize images to a consistent size\n",
    "        transforms.ToTensor(),        # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
    "    ])\n",
    "\n",
    "    # Load datasets using ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(root=os.path.join(root_dir, 'train'), transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=os.path.join(root_dir, 'test'), transform=transform)\n",
    "    validation_dataset = datasets.ImageFolder(root=os.path.join(root_dir, 'validation'), transform=transform)\n",
    "\n",
    "    # Create data loaders for train, test, and validation datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, validation_loader, train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c904ef1-666a-485a-9931-c9dd2b817029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        # Initialize tqdm for the training loop\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # Clear the gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update the weights\n",
    "            running_loss += loss.item()\n",
    "            # Update tqdm with the loss\n",
    "            progress_bar.set_postfix(loss=running_loss / len(train_loader))\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ab9fb-c798-4af5-9721-3c364131d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in tqdm(data_loader, desc='Evaluating', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8874cc-6dc8-4e52-b795-9cb9b61b04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to predict a single image\n",
    "def predict_single_image(model, image_path, transform, device, class_names):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Preprocess and add batch dimension\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        output = model(image_tensor)  # Forward pass\n",
    "        _, predicted = torch.max(output, 1)  # Get the predicted class index\n",
    "    return class_names[predicted.item()]  # Return the predicted class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc41f0-c562-4ffe-b88e-80d175f74907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'num_filters': [32, 64],\n",
    "    'kernel_size': [3, 5],\n",
    "    'dropout_rate': [0.0, 0.2],\n",
    "    'num_units1': [32, 64],\n",
    "    'num_units2': [32, 64],\n",
    "    'learning_rate': [1e-4, 1e-2],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "param_combinations = list(itertools.product(\n",
    "    param_grid['num_filters'],\n",
    "    param_grid['kernel_size'],\n",
    "    param_grid['dropout_rate'],\n",
    "    param_grid['num_units1'],\n",
    "    param_grid['num_units2'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['batch_size']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81952c3-fc58-4912-b1ef-d57bfdfb24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indexes = random.sample(range(len(param_combinations)), 10)\n",
    "random_param_combinations = [param_combinations[i] for i in random_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4572ee3-c1f2-4855-a2d1-3ad8bd796677",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb9b87-c8ab-4c54-90c5-f4588248387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "print(\"Testing \" + str(len(random_param_combinations)) + \" combination of hyperparameters.\")\n",
    "combination_num = 1\n",
    "for params in random_param_combinations:\n",
    "    print(\"00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\")\n",
    "    print(\"Combination - \" + str(combination_num))\n",
    "    print(params)\n",
    "    num_filters, kernel_size, dropout_rate, num_units1, num_units2, learning_rate, batch_size = params\n",
    "\n",
    "    model = CNN(num_filters, kernel_size, dropout_rate, num_units1, num_units2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Load data and get class names\n",
    "    root_dir = \"D:/Study/COMP 6721/COMP6721-project/Dataset\"\n",
    "    train_loader, test_loader, validation_loader, class_names = load_data(root_dir, batch_size)\n",
    "\n",
    "    num_epochs = 10\n",
    "    model = train_model(model, train_loader, criterion, optimizer, device, num_epochs)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    validation_accuracy = evaluate_model(model, validation_loader, device)\n",
    "    print(f\"Params: {params}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "    combination_num += 1\n",
    "\n",
    "    if validation_accuracy > best_accuracy:\n",
    "        best_accuracy = validation_accuracy\n",
    "        best_params = params\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "# Save the best model's state dictionary to a file\n",
    "timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "torch.save(best_model, f'best_model_{timestamp}.pth')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7dab9-3912-45b6-bc47-5bd1f66e6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and evaluate on the test set\n",
    "best_model_loaded = CNN(best_params[0], best_params[1], best_params[2], best_params[3], best_params[4]).to(device)\n",
    "best_model_loaded.load_state_dict(best_model)\n",
    "test_accuracy = evaluate_model(best_model_loaded, test_loader, device)\n",
    "print(f\"Best Parameters: {best_params}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5da0be-1c90-46f4-8cdf-7f26fce0443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "def visualize_model(model, input_size=(3, 64, 64)):\n",
    "    x = torch.randn(1, *input_size).to(next(model.parameters()).device)\n",
    "    y = model(x)\n",
    "    make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)])).render(\"cnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706aeac-0481-4b6e-a7eb-dbf4dae74294",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(best_model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e15541-1839-490c-acd2-e6c60809f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of predicting a single image\n",
    "image_path_to_predict = \"D:/Study/COMP 6721/COMP6721-project/Phase2-Notebooks/test_images/00000070.jpg\"  # Replace with your image path\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "predicted_class = predict_single_image(best_model_loaded, image_path_to_predict, transform, device, class_names)\n",
    "print(f'Predicted class for {image_path_to_predict}: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245baf4f-0655-4950-9f47-b609ac6c182b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
